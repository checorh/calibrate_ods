{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "input_path = '/Users/rodrse/Downloads/calibrate_ods_carolina/network/SFO/metadata/'\n",
    "simulation_setup = \"lowCongestion_5minDemand\"\n",
    "\n",
    "config = json.load(open(input_path + \"/config.json\"))\n",
    "config[\"NETWORK\"] = Path(config[\"NETWORK\"])\n",
    "config[\"SUMO\"] = Path(config[\"SUMO\"])\n",
    "# [CO] updated simulation setup to use OD scenario #4: 42-dimensional  + low congestion.\n",
    "#sim_setup = json.load(open(input_path + \"simulation_setups_co_gt.json\"))\n",
    "sim_setup = json.load(open(input_path + f\"simulation_setups_{simulation_setup}.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding warmup period of 15min\n",
    "# so you need to simulate for at least 15min now. \n",
    "warm_up_sec = 15*60 # duration in seconds\n",
    "\n",
    "# duration, in seconds, of each edge simulation output statistics\n",
    "# this value should be consistent with what is defined in additional.add.xml\n",
    "edge_stats_freq = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1.],\n",
       "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Main simulator function\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_sumo(config, sim_setup, prefix_input, prefix_output):\n",
    "    od2trips_cmd = (\n",
    "        #f\"od2trips --no-step-log  --spread.uniform \"\n",
    "        f\"od2trips  --spread.uniform \"\n",
    "        #Loads TAZ (districts)\n",
    "        f\"--taz-files {config['NETWORK']}/{sim_setup['taz']} \" \n",
    "        # Loads O/D-matrix in tazRelation format fromFILE(s)\n",
    "        f\"--tazrelation-files {config['NETWORK']}/{sim_setup[f'{prefix_input}_od']} \"\n",
    "        # Writes trip definitions into FILE\n",
    "        f\"-o {config['NETWORK']}/{prefix_output}_{sim_setup['prefix_sim_run']}_od_trips.trips.xml \" \n",
    "    )\n",
    "\n",
    "    # Run SUMO to generate outputs\n",
    "    sumo_run = (\n",
    "        # Prefix which is applied to all output files. \n",
    "        f\"sumo --output-prefix {prefix_output}_{sim_setup['prefix_sim_run']}_ \" \n",
    "        # Do not check whether routes are connected\n",
    "        f\"--ignore-route-errors=true \"\n",
    "        # Load road network description from FILE\n",
    "        f\"--net-file={config['NETWORK']/sim_setup['net']} \"\n",
    "        # Load routes descriptions from FILE(s)\n",
    "        f\"--routes={config['NETWORK']}/{prefix_output}_{sim_setup['prefix_sim_run']}_od_trips.trips.xml \"\n",
    "        #  -b Defines the begin time in seconds; The simulation starts at this time\n",
    "        # -e Defines the end time in seconds; The simulation ends at this time\n",
    "        f\"-b {sim_setup['start_sim_sec']} -e {sim_setup['end_sim_sec']} \"\n",
    "        # Load further descriptions from FILE(s)\n",
    "        f\"--additional-files {config['NETWORK']/sim_setup['add']} \"\n",
    "        f\"--duration-log.statistics \"\n",
    "        f\"--xml-validation never \"\n",
    "        # Save single vehicle route info into FILE\n",
    "        f\"--vehroutes {config['NETWORK']}/routes.vehroutes.xml \"\n",
    "        f\"--verbose \"\n",
    "        # Disables output of warnings\n",
    "        f\"--no-warnings \"\n",
    "        # Faster simulation (i.e. less detailed)\n",
    "        f\"--mesosim true \"\n",
    "    \n",
    "    )\n",
    "        # f\"--seed {seed}\"\n",
    "\n",
    "    try:\n",
    "        print(od2trips_cmd)\n",
    "        os.system(od2trips_cmd)\n",
    "    except:\n",
    "        print(\"Unable to create trips file\")\n",
    "    else:\n",
    "        print(\"###### Running SUMO #######\")\n",
    "        print(sumo_run)\n",
    "        os.system(sumo_run)\n",
    "\n",
    "\n",
    "def parse_loop_data_xml_to_pandas(config: dict,loop_file: dict) -> DataFrame: \n",
    "    \"\"\"Read the Loop Detectors Data: Each SUMO run produces a file with the\n",
    "    traffic counts. This function reads the corresponding traffic counts file\n",
    "    averages across simulation replications\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    output_file =(config[\"NETWORK\"] / \"loopOutputs.csv\")\n",
    "    ## See output explanation:\n",
    "    # https://sumo.dlr.de/docs/Simulation/Output/Lane-_or_Edge-based_Traffic_Measures.html#generated_output\n",
    "\n",
    "    data2csv = (\n",
    "        f\"python {config['SUMO']}/tools/xml/xml2csv.py \"\n",
    "        f\"{loop_file} \"\n",
    "        f\"-o {output_file}\"\n",
    "        )\n",
    "    \n",
    "    os.system(data2csv)\n",
    "    \n",
    "    df_trips = pd.read_csv(output_file, sep=\";\", header=0)\n",
    "\n",
    "    # edge flow in vehicles per hour\n",
    "    ## edge speed is given in m/s\n",
    "    ## edge_density is given in no. of vehicles/km\n",
    "\n",
    "    df_trips['interval_nVehContrib'] = df_trips['edge_arrived'] + df_trips['edge_left']\n",
    "    #df_trips['interval_nVehContrib'] = 3.6*df_trips['edge_speed']*df_trips['edge_density']\n",
    "\n",
    "\n",
    "    #df_trips['EdgeID'] = df_trips['edge_id']\n",
    "\n",
    "    # edge speed is given in m/s\n",
    "    # computed only for edges that have departed flow\n",
    "    df_trips['interval_harmonicMeanSpeed'] = df_trips[df_trips['interval_nVehContrib']>0]['edge_speed']\n",
    "    \n",
    "    # exclude warm-up period\n",
    "    df_trips = df_trips[df_trips['interval_begin']>warm_up_sec]\n",
    "\n",
    "    # aggregate the rest of the time intervals\n",
    "    df_trips = df_trips[df_trips['interval_begin']>warm_up_sec]\n",
    "    df_agg = df_trips.groupby(by=['edge_id'], as_index=False).agg(\n",
    "        {'interval_nVehContrib':np.sum, 'interval_harmonicMeanSpeed':np.mean})\n",
    "\n",
    "\n",
    "    return df_agg, df_trips\n",
    "\n",
    "\n",
    "\n",
    "def compute_nrmse_counts_one_edge(df_true, df_simulated,GT_edge_id):\n",
    "    # Merge simulated output with ground truth\n",
    "    df1 = df_true\\\n",
    "        .merge(df_simulated, on=['edge_id'],\n",
    "        suffixes=('_GT', '_sim'), how='left')\n",
    "    \n",
    "    # only consider GT_edge_id of interest\n",
    "    df1 = df1[df1['edge_id']==GT_edge_id]\n",
    "\n",
    "    df1['interval_nVehContrib_sim'] = df1['interval_nVehContrib_sim'].fillna(0)\n",
    "        \n",
    "    df1['diff_square'] = (\n",
    "        df1['interval_nVehContrib_GT'] - df1['interval_nVehContrib_sim']\n",
    "        )**2\n",
    "    \n",
    "    n = df1.shape[0]\n",
    "    print(n)\n",
    "    print(df_true.shape[0])\n",
    "    print(df_simulated.shape[0])\n",
    "    RMSN = np.sqrt(n*(df1['diff_square'].sum()))/df1['interval_nVehContrib_GT'].sum()\n",
    "\n",
    "    return RMSN\n",
    "\n",
    "\n",
    "def compute_nrmse_counts_all_edges(df_true, df_simulated):\n",
    "    # Merge simulated output with ground truth\n",
    "    df1 = df_true\\\n",
    "        .merge(df_simulated, on=['edge_id'],\n",
    "        suffixes=('_GT', '_sim'), how='left')\n",
    "    \n",
    "    df1['interval_nVehContrib_sim'] = df1['interval_nVehContrib_sim'].fillna(0)\n",
    "        \n",
    "    df1['diff_square'] = (\n",
    "        df1['interval_nVehContrib_GT'] - df1['interval_nVehContrib_sim']\n",
    "        )**2\n",
    "    \n",
    "    n = df1.shape[0]\n",
    "    print(n)\n",
    "    print(df_true.shape[0])\n",
    "    print(df_simulated.shape[0])\n",
    "    RMSN = np.sqrt(n*(df1['diff_square'].sum()))/df1['interval_nVehContrib_GT'].sum()\n",
    "\n",
    "    return RMSN\n",
    "\n",
    "\n",
    "\n",
    "def generate_od_xml(x, config, sim_setup):\n",
    "\n",
    "    init_od_path = f\"{config['NETWORK']}/{sim_setup['init_od']}\"\n",
    "\n",
    "    if Path(init_od_path).is_file():\n",
    "        print(\"Reading:\",init_od_path)\n",
    "        tree = ET.parse(init_od_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for i,x in enumerate(x):\n",
    "            root[0][i].attrib[\"count\"] = str(np.round(x,4))\n",
    "            \n",
    "    file_name = f\"{config['NETWORK']}/{sim_setup['current_od']}\"\n",
    "    print('Saving: '+file_name)\n",
    "    tree.write(file_name)\n",
    "## Find upper and lower bounds\n",
    "import pandas as pd\n",
    "\n",
    "def transform_od_xml_to_pandas(file_path_xml):\n",
    "\n",
    "    tree = ET.parse(file_path_xml)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    od_mat = []\n",
    "    for taz_elem in root.iter('tazRelation'):\n",
    "        od_mat.append([taz_elem.get('from'), taz_elem.get('to'), int(taz_elem.get('count'))])\n",
    "    \n",
    "    df_od = pd.DataFrame(od_mat, columns=['tazFrom', 'tazTo', 'tazCount'])\n",
    "\n",
    "    return df_od\n",
    "\n",
    "file_path_xml = str(config['NETWORK']) + \"/\" + sim_setup['gt_od']\n",
    "df_gt = transform_od_xml_to_pandas(file_path_xml)\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "### Declare search space\n",
    "# dimensionality of input space\n",
    "\n",
    "dim_od = df_gt.shape[0]\n",
    "\n",
    "bounds = torch.tensor([\n",
    "    [ df_gt[ 'tazCount'].min() for _ in range(dim_od)],\n",
    "    [ df_gt[ 'tazCount'].max() for _ in range(dim_od)]\n",
    "], device=device, dtype=dtype) \n",
    "\n",
    "bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain ground-truth value\n",
    "loop_stats_df_gt, _ = parse_loop_data_xml_to_pandas(\n",
    "    config,\n",
    "    loop_file = f\"/Users/rodrse/Downloads/calibrate_ods_carolina/network/SFO/gt_{sim_setup['prefix_sim_run']}_edge_data_SFO.xml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from botorch.utils.transforms import normalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.models.transforms import Standardize\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintain the TuRBO state\n",
    "\n",
    "https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf\n",
    "\n",
    "TuRBO needs to maintain a state, which includes the length of the trust region, success and failure counters, success and failure tolerance, etc.\n",
    "\n",
    "In this tutorial we store the state in a dataclass and update the state of TuRBO after each batch evaluation.\n",
    "\n",
    "Note: These settings assume that the domain has been scaled to [0,1]𝑑\n",
    " and that the same batch size is used for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5**7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 3  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new batch\n",
    "\n",
    "Given the current state and a probabilistic (GP) model built from observations X and Y, we generate a new batch of points.\n",
    "\n",
    "This method works on the domain [0,1]𝑑\n",
    ", so make sure to not pass in observations from the true domain. unnormalize is called before the true function is evaluated which will first map the points back to the original domain.\n",
    "\n",
    "We support either TS and qEI which can be specified via the acqf argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size,\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\", \"ei\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the TR to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = torch.rand(n_candidates, dim, dtype=dtype, device=device) <= prob_perturb\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask\n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        thompson_sampling = MaxPosteriorSampling(model=model, replacement=False)\n",
    "        with torch.no_grad():  # We don't need gradients when using TS\n",
    "            X_next = thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "    elif acqf == \"ei\":\n",
    "        ei = qExpectedImprovement(model, train_Y.max(), maximize=True)\n",
    "        X_next, acq_value = optimize_acqf(\n",
    "            ei,\n",
    "            bounds=torch.stack([tr_lb, tr_ub]),\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "        )\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop\n",
    "\n",
    "This simple loop runs one instance of TuRBO-1 with Thompson sampling until convergence.\n",
    "\n",
    "TuRBO-1 is a local optimizer that can be used for a fixed evaluation budget in a multi-start fashion. Once TuRBO converges, state[\"restart_triggered\"] will be set to true and the run should be aborted. If you want to run more evaluations with TuRBO, you simply generate a new set of initial points and then keep generating batches until convergence or when the evaluation budget has been exceeded. It's important to note that evaluations from previous instances are discarded when TuRBO restarts.\n",
    "\n",
    "NOTE: We use a SingleTaskGP with a noise constraint to keep the noise from getting too large as the problem is noise-free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial sampling locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 43)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read initial data\n",
    "df_0 = pd.read_csv(f\"./{simulation_setup}_initial_data_bo.csv\")\n",
    "df_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_input = \"current\"\n",
    "prefix_output = \"current\"\n",
    "loop_file = f\"/Users/rodrse/Downloads/calibrate_ods_carolina/network/SFO/{prefix_output}_{sim_setup['prefix_sim_run']}_edge_data_SFO.xml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### BO iteration=1 ###########\n",
      "##### best_value=-0.816323829013397 #####\n",
      "Fitting GP model...\n",
      "Optimizing acquisition function...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# Create a batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOptimizing acquisition function...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     new_x_turbo \u001b[39m=\u001b[39m generate_batch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m         state\u001b[39m=\u001b[39;49mstate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         X\u001b[39m=\u001b[39;49mtrain_X_norm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         Y\u001b[39m=\u001b[39;49mtrain_Y,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         n_candidates\u001b[39m=\u001b[39;49mN_CANDIDATES,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         num_restarts\u001b[39m=\u001b[39;49mNUM_RESTARTS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         raw_samples\u001b[39m=\u001b[39;49mRAW_SAMPLES,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         acqf\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mei\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# \"ts\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m x_i \u001b[39m=\u001b[39m unnormalize(new_x_turbo,bounds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m x_i \u001b[39m=\u001b[39m x_i\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_batch\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     state,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model,  \u001b[39m# GP model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     acqf\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mts\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# \"ei\" or \"ts\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39massert\u001b[39;00m acqf \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mts\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mei\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39massert\u001b[39;00m X\u001b[39m.\u001b[39mmin() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m X\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mall(torch\u001b[39m.\u001b[39misfinite(Y))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m n_candidates \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rodrse/Downloads/calibrate_ods_carolina/notebooks/bayesian_optimization/3_bo_trust_region.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         n_candidates \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39m5000\u001b[39m, \u001b[39mmax\u001b[39m(\u001b[39m2000\u001b[39m, \u001b[39m200\u001b[39m \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "dim=dim_od\n",
    "state = TurboState(dim, BATCH_SIZE)\n",
    "\n",
    "SMOKE_TEST  = False\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 512 if not SMOKE_TEST else 4\n",
    "N_CANDIDATES = min(5000, max(2000, 200 * dim)) if not SMOKE_TEST else 4\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Run loop\n",
    "\n",
    "NITER = 50\n",
    "best_value = []\n",
    "\n",
    "# Data frame of current training data\n",
    "df_training = df_0\n",
    "df_training[\"bo_iteration\"] = 0\n",
    "\n",
    "\n",
    "for i in range(NITER):\n",
    "\n",
    "    print(f\"########### BO iteration={i+1} ###########\")\n",
    "\n",
    "    # Obtain sampling locations x\n",
    "    train_X = torch.from_numpy(\n",
    "        df_training[[col for col in df_training.columns if \"x\" in col]].values\n",
    "    )    \n",
    "\n",
    "    # Normalize\n",
    "    train_X_norm = normalize(train_X,bounds)\n",
    "\n",
    "    # Obtain reponse data\n",
    "    train_Y = -torch.from_numpy(df_training[[\"loss\"]].values) # Take negative\n",
    "\n",
    "    ###\n",
    "    #Obtain next sampling location given current training data\n",
    "    ###\n",
    "\n",
    "    # best value so far\n",
    "    best_y = train_Y.max()\n",
    "    best_value.append(best_y)\n",
    "    print(f\"##### best_value={best_y} #####\")\n",
    "\n",
    "\n",
    "    # Update state\n",
    "    state = update_state(state=state, Y_next=train_Y)\n",
    "\n",
    "    # Fit model\n",
    "    print(\"Fitting GP model...\")\n",
    "\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    # Use the same lengthscale prior as in the TuRBO paper\n",
    "    covar_module = ScaleKernel(  \n",
    "        MaternKernel(\n",
    "            nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0)\n",
    "        )\n",
    "    )\n",
    "    model = SingleTaskGP(\n",
    "        train_X_norm, train_Y, \n",
    "        covar_module=covar_module, likelihood=likelihood, \n",
    "        outcome_transform=Standardize(m=1)\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    # Do the fitting and acquisition function optimization inside the Cholesky context\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        # Fit the model\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "        # Create a batch\n",
    "        print(\"Optimizing acquisition function...\")\n",
    "        new_x_turbo = generate_batch(\n",
    "            state=state,\n",
    "            model=model,\n",
    "            X=train_X_norm,\n",
    "            Y=train_Y,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_candidates=N_CANDIDATES,\n",
    "            num_restarts=NUM_RESTARTS,\n",
    "            raw_samples=RAW_SAMPLES,\n",
    "            acqf=\"ei\", # \"ts\"\n",
    "        )\n",
    "\n",
    "    x_i = unnormalize(new_x_turbo,bounds)\n",
    "    x_i = x_i.cpu().detach().numpy()\n",
    "\n",
    "    # Print current status\n",
    "    print(\n",
    "        f\"Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Next {BATCH_SIZE} sampling locations:{x_i}.\")\n",
    "\n",
    "    # Sample simulator\n",
    "    batch_data_i = []\n",
    "    for j in range(BATCH_SIZE):\n",
    "        loss = []\n",
    "        print(f\"########### Sampling location={j+1} ###########\")\n",
    "\n",
    "        # Generate OD\n",
    "        print(f\"Generating new OD...\")\n",
    "        generate_od_xml(x_i[j], config, sim_setup)\n",
    "\n",
    "        # Query simulator\n",
    "        print(f\"Querying simulator...\")\n",
    "        run_sumo(config, sim_setup, prefix_input, prefix_output)\n",
    "\n",
    "        # Compute metrics\n",
    "        print(f\"Computing metrics...\")\n",
    "        loop_stats_df_current, _ = parse_loop_data_xml_to_pandas(config,loop_file)\n",
    "\n",
    "        # Compute loss with respect to gt\n",
    "        y_i = compute_nrmse_counts_all_edges(loop_stats_df_gt, loop_stats_df_current)\n",
    "\n",
    "        # Parse training data\n",
    "        df_j = pd.DataFrame(x_i[j].reshape(1,dim_od),\n",
    "                            columns = [f\"x_{i+1}\" for i in range(dim_od)])\n",
    "        df_j['loss'] = y_i\n",
    "        batch_data_i.append(df_j)\n",
    "\n",
    "    df_i = pd.concat(batch_data_i)\n",
    "    df_i[\"bo_iteration\"] = i+1\n",
    "\n",
    "    df_training = pd.concat([df_training,df_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>...</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>loss</th>\n",
       "      <th>bo_iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.611547</td>\n",
       "      <td>2.273848</td>\n",
       "      <td>2.592454</td>\n",
       "      <td>2.890642</td>\n",
       "      <td>2.450092</td>\n",
       "      <td>1.219814</td>\n",
       "      <td>1.802633</td>\n",
       "      <td>1.915650</td>\n",
       "      <td>2.646243</td>\n",
       "      <td>1.738624</td>\n",
       "      <td>...</td>\n",
       "      <td>2.335956</td>\n",
       "      <td>1.307782</td>\n",
       "      <td>1.438092</td>\n",
       "      <td>2.850333</td>\n",
       "      <td>1.238476</td>\n",
       "      <td>1.039035</td>\n",
       "      <td>1.318928</td>\n",
       "      <td>2.944241</td>\n",
       "      <td>0.908423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.054296</td>\n",
       "      <td>1.833210</td>\n",
       "      <td>1.453317</td>\n",
       "      <td>1.793119</td>\n",
       "      <td>1.011128</td>\n",
       "      <td>2.004998</td>\n",
       "      <td>2.686413</td>\n",
       "      <td>2.237398</td>\n",
       "      <td>1.528236</td>\n",
       "      <td>2.326696</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424358</td>\n",
       "      <td>2.134762</td>\n",
       "      <td>2.469135</td>\n",
       "      <td>1.111838</td>\n",
       "      <td>2.591812</td>\n",
       "      <td>2.983859</td>\n",
       "      <td>2.279002</td>\n",
       "      <td>1.007318</td>\n",
       "      <td>0.861131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.964279</td>\n",
       "      <td>2.620410</td>\n",
       "      <td>2.140783</td>\n",
       "      <td>2.396731</td>\n",
       "      <td>1.755001</td>\n",
       "      <td>2.723452</td>\n",
       "      <td>1.251961</td>\n",
       "      <td>2.872213</td>\n",
       "      <td>1.053167</td>\n",
       "      <td>2.633721</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619655</td>\n",
       "      <td>2.567077</td>\n",
       "      <td>1.827200</td>\n",
       "      <td>2.211073</td>\n",
       "      <td>1.678217</td>\n",
       "      <td>1.566791</td>\n",
       "      <td>1.522626</td>\n",
       "      <td>1.558986</td>\n",
       "      <td>0.879384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.405075</td>\n",
       "      <td>1.052806</td>\n",
       "      <td>1.779996</td>\n",
       "      <td>1.291395</td>\n",
       "      <td>2.690059</td>\n",
       "      <td>1.500826</td>\n",
       "      <td>2.133818</td>\n",
       "      <td>1.038685</td>\n",
       "      <td>2.186873</td>\n",
       "      <td>1.299067</td>\n",
       "      <td>...</td>\n",
       "      <td>2.640659</td>\n",
       "      <td>1.990706</td>\n",
       "      <td>2.859328</td>\n",
       "      <td>1.949325</td>\n",
       "      <td>2.027621</td>\n",
       "      <td>2.387591</td>\n",
       "      <td>2.574267</td>\n",
       "      <td>2.498046</td>\n",
       "      <td>0.885530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.089331</td>\n",
       "      <td>2.829342</td>\n",
       "      <td>1.248254</td>\n",
       "      <td>1.219339</td>\n",
       "      <td>2.806553</td>\n",
       "      <td>1.466984</td>\n",
       "      <td>1.113728</td>\n",
       "      <td>2.643030</td>\n",
       "      <td>1.432918</td>\n",
       "      <td>2.094678</td>\n",
       "      <td>...</td>\n",
       "      <td>1.225980</td>\n",
       "      <td>2.325162</td>\n",
       "      <td>2.139201</td>\n",
       "      <td>1.683845</td>\n",
       "      <td>1.978403</td>\n",
       "      <td>2.144211</td>\n",
       "      <td>2.831678</td>\n",
       "      <td>1.425993</td>\n",
       "      <td>0.816324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2       x_3       x_4       x_5       x_6       x_7  \\\n",
       "0  2.611547  2.273848  2.592454  2.890642  2.450092  1.219814  1.802633   \n",
       "1  1.054296  1.833210  1.453317  1.793119  1.011128  2.004998  2.686413   \n",
       "2  1.964279  2.620410  2.140783  2.396731  1.755001  2.723452  1.251961   \n",
       "3  2.405075  1.052806  1.779996  1.291395  2.690059  1.500826  2.133818   \n",
       "4  2.089331  2.829342  1.248254  1.219339  2.806553  1.466984  1.113728   \n",
       "\n",
       "        x_8       x_9      x_10  ...      x_35      x_36      x_37      x_38  \\\n",
       "0  1.915650  2.646243  1.738624  ...  2.335956  1.307782  1.438092  2.850333   \n",
       "1  2.237398  1.528236  2.326696  ...  1.424358  2.134762  2.469135  1.111838   \n",
       "2  2.872213  1.053167  2.633721  ...  1.619655  2.567077  1.827200  2.211073   \n",
       "3  1.038685  2.186873  1.299067  ...  2.640659  1.990706  2.859328  1.949325   \n",
       "4  2.643030  1.432918  2.094678  ...  1.225980  2.325162  2.139201  1.683845   \n",
       "\n",
       "       x_39      x_40      x_41      x_42      loss  bo_iteration  \n",
       "0  1.238476  1.039035  1.318928  2.944241  0.908423             0  \n",
       "1  2.591812  2.983859  2.279002  1.007318  0.861131             0  \n",
       "2  1.678217  1.566791  1.522626  1.558986  0.879384             0  \n",
       "3  2.027621  2.387591  2.574267  2.498046  0.885530             0  \n",
       "4  1.978403  2.144211  2.831678  1.425993  0.816324             0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_plot = df_training.query('bo_iteration>0')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = df_plot['bo_iteration']\n",
    "y = df_plot['loss']\n",
    "\n",
    "plt.plot(x, best_value)\n",
    "#plt.legend(title='Parameter where:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.828104\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BoTorch",
   "language": "python",
   "name": "botorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "609ae87f148291bad4d4d4812b8a406436d6ec29273407f95ac86e73fc385d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
